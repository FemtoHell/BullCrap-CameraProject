import cv2
import mediapipe as mp
import numpy as np
import os
import time

# Kh·ªüi t·∫°o MediaPipe
mp_hands = mp.solutions.hands
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.5
)

face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# === NH·∫¨N DI·ªÜN ƒê·ªòNG T√ÅC TAY ===
def detect_hand_gesture(hand_landmarks, hand_label):
    """Nh·∫≠n di·ªán ƒë·ªông t√°c tay"""
    landmarks = hand_landmarks.landmark
    
    # Ki·ªÉm tra t·ª´ng ng√≥n
    thumb_tip = landmarks[4]
    thumb_ip = landmarks[3]
    
    if hand_label == "Right":
        thumb_up = thumb_tip.x < thumb_ip.x
    else:
        thumb_up = thumb_tip.x > thumb_ip.x
    
    index_up = landmarks[8].y < landmarks[6].y
    middle_up = landmarks[12].y < landmarks[10].y
    ring_up = landmarks[16].y < landmarks[14].y
    pinky_up = landmarks[20].y < landmarks[18].y
    
    fingers = [thumb_up, index_up, middle_up, ring_up, pinky_up]
    count = sum(fingers)
    
    # C√°c ƒë·ªông t√°c c·ª• th·ªÉ
    if count == 1 and index_up:
        return "point_one"  # Ch·ªâ 1 ng√≥n tr·ªè
    elif count == 5:
        return "open_hand"  # T·∫•t c·∫£ ng√≥n
    else:
        return None

def check_hands_cover_face(hand_results, face_results):
    """Ki·ªÉm tra 2 tay c√≥ che m·∫∑t kh√¥ng"""
    if face_results.multi_face_landmarks is None:
        return False
    
    if hand_results.multi_hand_landmarks is None:
        return False
    
    if len(hand_results.multi_hand_landmarks) < 2:
        return False
    
    # L·∫•y v·ªã tr√≠ m·∫∑t
    face_landmarks = face_results.multi_face_landmarks[0]
    face_center_x = face_landmarks.landmark[1].x
    face_center_y = face_landmarks.landmark[1].y
    
    # Ki·ªÉm tra c·∫£ 2 tay g·∫ßn m·∫∑t
    hands_near = 0
    for hand_landmarks in hand_results.multi_hand_landmarks:
        hand_center_x = hand_landmarks.landmark[9].x
        hand_center_y = hand_landmarks.landmark[9].y
        
        distance = np.sqrt((face_center_x - hand_center_x)**2 + (face_center_y - hand_center_y)**2)
        if distance < 0.2:
            hands_near += 1
    
    return hands_near >= 2

# === NH·∫¨N DI·ªÜN BI·ªÇU C·∫¢M M·∫∂T ===
def detect_face_expression(face_landmarks):
    """Nh·∫≠n di·ªán bi·ªÉu c·∫£m m·∫∑t"""
    if face_landmarks is None:
        return None
    
    landmarks = face_landmarks.landmark
    
    # T√≠nh ƒë·ªô m·ªü mi·ªáng
    upper_lip = landmarks[13]
    lower_lip = landmarks[14]
    mouth_left = landmarks[61]
    mouth_right = landmarks[291]
    
    mouth_height = abs(upper_lip.y - lower_lip.y)
    mouth_width = abs(mouth_left.x - mouth_right.x)
    mouth_ratio = mouth_height / (mouth_width + 0.001)
    
    # Ph√¢n lo·∫°i
    if mouth_ratio > 0.5:
        return "mouth_wide_open"  # H√° mi·ªáng r·∫•t to
    elif mouth_ratio > 0.3:
        return "mouth_open"  # L√® l∆∞·ª°i / mi·ªáng m·ªü v·ª´a
    elif mouth_ratio > 0.15:
        return "smile"  # C∆∞·ªùi
    else:
        return "neutral"

# === MAPPING ƒê·ªòNG T√ÅC ‚Üí ·∫¢NH CON KH·ªà ===
GESTURE_TO_IMAGE = {
    # ƒê·ªông t√°c 1: Gi∆° 1 ng√≥n + C∆∞·ªùi
    ("point_one", "smile"): 1,
    
    # ƒê·ªông t√°c 2: Che m·∫∑t
    ("cover_face", "any"): 2,
    
    # ƒê·ªông t√°c 3: L√® l∆∞·ª°i + Gi∆° 1 ng√≥n
    ("point_one", "mouth_open"): 3,
    
    # ƒê·ªông t√°c 4: L√® l∆∞·ª°i (kh√¥ng c·∫ßn ƒë·ªông t√°c tay c·ª• th·ªÉ)
    ("any", "mouth_open"): 4,
    
    # ƒê·ªông t√°c 5: H√° mi·ªáng s·ªëc
    ("any", "mouth_wide_open"): 5,
    
    # ƒê·ªông t√°c 6: C∆∞·ªùi + Gi∆° 1 ng√≥n (gi·ªëng 1, c√≥ th·ªÉ l√† backup)
    ("point_one", "neutral"): 6,
}

# Load ·∫£nh
def load_image_for_number(number):
    image_extensions = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']
    
    for ext in image_extensions:
        image_path = f'images/{number}{ext}'
        if os.path.exists(image_path):
            img = cv2.imread(image_path)
            if img is not None:
                return cv2.resize(img, (600, 600))
    return None

# T·∫°o ·∫£nh m·∫∑c ƒë·ªãnh
def create_default_image(width=600, height=600):
    img = np.zeros((height, width, 3), dtype=np.uint8)
    img[:] = (60, 60, 60)
    
    cv2.putText(img, "Lam dong tac de xem con khi!", (30, 300), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (200, 200, 200), 2)
    cv2.putText(img, "Try: Cuoi, Le luoi, Che mat, Chi 1 ngon...", (30, 350), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (150, 150, 150), 2)
    
    return img

# Pre-load ·∫£nh
print("ƒêang load ·∫£nh...")
loaded_images = {}

for i in range(1, 7):
    img = load_image_for_number(i)
    if img is not None:
        loaded_images[i] = img
        print(f"‚úì ƒê√£ load: images/{i}.jpg")
    else:
        print(f"‚úó Kh√¥ng t√¨m th·∫•y ·∫£nh {i}")

# Kh·ªüi t·∫°o camera
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

if not cap.isOpened():
    print("L·ªñI: Kh√¥ng th·ªÉ m·ªü camera!")
    exit()

# Bi·∫øn hi·ªÉn th·ªã
current_image = create_default_image()
last_gesture_time = time.time()
stable_gesture_key = None
stable_count = 0

print("\n" + "="*60)
print("=== G∆Ø∆†NG PH·∫¢N CHI·∫æU CON KH·ªà ===")
print("="*60)
print("L√†m ƒë·ªông t√°c ‚Üí Con kh·ªâ l√†m theo!")
print("Th·ª≠ c√°c ƒë·ªông t√°c:")
print("  ‚Ä¢ C∆∞·ªùi üòä")
print("  ‚Ä¢ L√® l∆∞·ª°i üòõ")
print("  ‚Ä¢ H√° mi·ªáng s·ªëc üò≤")
print("  ‚Ä¢ Che m·∫∑t üôà")
print("  ‚Ä¢ Gi∆° 1 ng√≥n tr·ªè üëÜ")
print("  ‚Ä¢ K·∫øt h·ª£p: C∆∞·ªùi + Gi∆° 1 ng√≥n")
print("  ‚Ä¢ K·∫øt h·ª£p: L√® l∆∞·ª°i + Gi∆° 1 ng√≥n")
print("\nNh·∫•n 'q' ƒë·ªÉ tho√°t")
print("="*60)

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        break
    
    frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # Nh·∫≠n di·ªán
    hand_results = hands.process(rgb_frame)
    face_results = face_mesh.process(rgb_frame)
    
    # V·∫Ω header
    cv2.rectangle(frame, (0, 0), (640, 100), (50, 50, 50), -1)
    
    # Ph√°t hi·ªán ƒë·ªông t√°c hi·ªán t·∫°i
    current_hand = None
    current_face = None
    
    # Ki·ªÉm tra che m·∫∑t tr∆∞·ªõc
    if check_hands_cover_face(hand_results, face_results):
        current_hand = "cover_face"
        current_face = "any"
    else:
        # Ph√°t hi·ªán m·∫∑t
        if face_results.multi_face_landmarks:
            face_landmarks = face_results.multi_face_landmarks[0]
            current_face = detect_face_expression(face_landmarks)
        
        # Ph√°t hi·ªán tay
        if hand_results.multi_hand_landmarks:
            for idx, hand_landmarks in enumerate(hand_results.multi_hand_landmarks):
                hand_label = hand_results.multi_handedness[idx].classification[0].label
                
                # V·∫Ω b√†n tay
                color = (0, 255, 0) if idx == 0 else (255, 0, 0)
                mp_drawing.draw_landmarks(
                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                    mp_drawing.DrawingSpec(color=color, thickness=2, circle_radius=2),
                    mp_drawing.DrawingSpec(color=color, thickness=2)
                )
                
                # Nh·∫≠n di·ªán ƒë·ªông t√°c
                gesture = detect_hand_gesture(hand_landmarks, hand_label)
                if gesture and current_hand is None:
                    current_hand = gesture
    
    # T√¨m ·∫£nh ph√π h·ª£p
    matched_image_number = None
    gesture_key = None
    
    if current_hand or current_face:
        # Th·ª≠ match ch√≠nh x√°c
        hand_key = current_hand if current_hand else "any"
        face_key = current_face if current_face else "any"
        gesture_key = (hand_key, face_key)
        
        if gesture_key in GESTURE_TO_IMAGE:
            matched_image_number = GESTURE_TO_IMAGE[gesture_key]
        else:
            # Th·ª≠ match v·ªõi "any"
            for (h, f), img_num in GESTURE_TO_IMAGE.items():
                if (h == hand_key or h == "any") and (f == face_key or f == "any"):
                    matched_image_number = img_num
                    gesture_key = (h, f)
                    break
    
    # C·∫≠p nh·∫≠t ·∫£nh v·ªõi ƒë·ªô ·ªïn ƒë·ªãnh
    if gesture_key == stable_gesture_key:
        stable_count += 1
    else:
        stable_gesture_key = gesture_key
        stable_count = 1
    
    # Ch·ªâ ƒë·ªïi ·∫£nh khi ƒë·ªông t√°c ·ªïn ƒë·ªãnh (gi·ªØ ~0.3 gi√¢y)
    if stable_count > 9 and matched_image_number and matched_image_number in loaded_images:  # 9 frames ‚âà 0.3s
        current_image = loaded_images[matched_image_number].copy()
        last_gesture_time = time.time()
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i
    status_hand = current_hand if current_hand else "Khong co"
    status_face = current_face if current_face else "Khong ro"
    
    gesture_names = {
        "point_one": "Chi 1 ngon",
        "open_hand": "Mo ban tay",
        "cover_face": "Che mat",
        "mouth_open": "Le luoi",
        "mouth_wide_open": "Ha mieng soc",
        "smile": "Cuoi",
        "neutral": "Binh thuong"
    }
    
    display_hand = gesture_names.get(status_hand, status_hand)
    display_face = gesture_names.get(status_face, status_face)
    
    cv2.putText(frame, f"Tay: {display_hand}", (15, 35), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
    cv2.putText(frame, f"Mat: {display_face}", (15, 70), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
    
    # Hi·ªÉn th·ªã s·ªë ·∫£nh ƒëang hi·ªán
    if matched_image_number:
        cv2.putText(frame, f"-> Anh {matched_image_number}", (400, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
    
    # Reset v·ªÅ m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ ƒë·ªông t√°c trong 3 gi√¢y
    if time.time() - last_gesture_time > 3.0 and not (current_hand or current_face):
        current_image = create_default_image()
    
    # Hi·ªÉn th·ªã
    cv2.imshow('Camera - Ban', frame)
    cv2.imshow('Con khi - Phan chieu', current_image)
    
    if 'windows_positioned' not in locals():
        cv2.setWindowProperty('Camera - Ban', cv2.WND_PROP_TOPMOST, 1)
        cv2.setWindowProperty('Con khi - Phan chieu', cv2.WND_PROP_TOPMOST, 1)
        windows_positioned = True
    
    # Ph√≠m tho√°t
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q') or key == 27:
        break

cap.release()
cv2.destroyAllWindows()
hands.close()
face_mesh.close()
print("\nƒê√£ tho√°t!")